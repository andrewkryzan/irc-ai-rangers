{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqzCK6iWy0Gw"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# YOLOv5 → Addax (Windows-safe .pt)  —  One-Cell Converter\n",
        "# ---------------------------------------------------------------\n",
        "# What this does:\n",
        "#  1) Mounts Google Drive\n",
        "#  2) Locates your YOLOv5 checkpoint (best.pt) on Drive\n",
        "#     - If not found, prompts for upload\n",
        "#  3) Loads the model safely (PyTorch 2.6+ compatible)\n",
        "#  4) Embeds class names, with BOTH int and string keys in model.names\n",
        "#     - Addax indexes names with \"0\" (str), so we ensure both 0 and \"0\" work\n",
        "#  5) Removes any pathlib.PosixPath objects (Windows unpickling fix)\n",
        "#  6) Strips training-only keys and saves a clean file:\n",
        "#         /MyDrive/AddaxExports/<stem>_addaxwin.pt\n",
        "#\n",
        "# Edit the CONFIG section below if your Drive layout is different.\n",
        "# This cell is self-contained and safe to share with clients.\n",
        "# ================================================================\n",
        "\n",
        "# ------------------------\n",
        "# CONFIG — EDIT HERE\n",
        "# ------------------------\n",
        "# (A) Where to find class names (dataset.yaml). If missing, we'll fallback to names in the model,\n",
        "#     and if those are missing, we'll use MANUAL_NAMES below.\n",
        "DATA_YAML  = \"/content/drive/MyDrive/combined_copy/dataset.yaml\"  # <-- YOUR example\n",
        "\n",
        "# (B) Where to look for the YOLOv5 checkpoint on Drive (your example paths, can edit)\n",
        "SEARCH_GLOB = \"/content/drive/MyDrive/yolov5_runs/train/*/weights/best.pt\"   # common YOLOv5 run path\n",
        "FALLBACK_PT = \"/content/drive/MyDrive/combined_copy/best.pt\"                 # optional fixed path\n",
        "\n",
        "# (C) If neither path finds a file, we will prompt for upload.\n",
        "#     You can also hardcode a full path here if preferred.\n",
        "\n",
        "# (D) If dataset.yaml is missing or unreadable, we use this list:\n",
        "MANUAL_NAMES = [\"Cyclist\", \"Equestrian\", \"Pedestrian\", \"Vehicle\"]  # <-- YOUR 4 classes\n",
        "\n",
        "# (E) Where to save the Addax-ready file on Drive:\n",
        "SAVE_DIR = \"/content/drive/MyDrive/AddaxExports\"                    # <-- YOUR example\n",
        "# ------------------------\n",
        "\n",
        "# ========== NO EDITS NEEDED BELOW THIS LINE ==========\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, sys, glob, shutil, time, yaml, torch, hashlib, pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "def log(msg): print(f\"[AddaxPT] {msg}\")\n",
        "\n",
        "# 1) Locate source checkpoint on Drive (or upload)\n",
        "hits = sorted(glob.glob(SEARCH_GLOB))\n",
        "SRC = hits[-1] if hits else (FALLBACK_PT if os.path.exists(FALLBACK_PT) else None)\n",
        "if not SRC:\n",
        "    from google.colab import files\n",
        "    log(\"No best.pt found on Drive. Please upload your YOLOv5 .pt now (e.g., best.pt).\")\n",
        "    up = files.upload()\n",
        "    SRC = next((f\"/content/{k}\" for k in up if k.lower().endswith(\".pt\")), None)\n",
        "    assert SRC, \"No .pt uploaded; cannot proceed.\"\n",
        "\n",
        "log(f\"Source: {SRC}  |  size: {round(os.path.getsize(SRC)/1e6, 2)} MB\")\n",
        "\n",
        "# Copy to local for faster/safer I/O\n",
        "local_pt = \"/content/best_local.pt\"\n",
        "shutil.copyfile(SRC, local_pt)\n",
        "log(f\"Copied to: {local_pt}\")\n",
        "\n",
        "# 2) Make sure YOLOv5 code is available (for attempt_load & class allowlisting)\n",
        "if not Path(\"/content/yolov5\").exists():\n",
        "    !git clone -q https://github.com/ultralytics/yolov5 /content/yolov5\n",
        "sys.path.append(\"/content/yolov5\")\n",
        "\n",
        "# Allowlist YOLOv5 DetectionModel for torch.load (PyTorch 2.6 safe loading)\n",
        "from models.yolo import DetectionModel\n",
        "try:\n",
        "    from torch.serialization import add_safe_globals\n",
        "    add_safe_globals([DetectionModel])\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 3) Resolve class names\n",
        "names = None\n",
        "if os.path.exists(DATA_YAML):\n",
        "    try:\n",
        "        with open(DATA_YAML, \"r\") as f:\n",
        "            data = yaml.safe_load(f) or {}\n",
        "        if isinstance(data.get(\"names\"), list) and data[\"names\"]:\n",
        "            names = [str(x) for x in data[\"names\"]]\n",
        "            log(f\"Loaded class names from dataset.yaml: {names}\")\n",
        "    except Exception as e:\n",
        "        log(f\"Warning: Failed to read dataset.yaml ({e}). Will fallback.\")\n",
        "if not names:\n",
        "    # we will try to read from model later; if still missing, use MANUAL_NAMES\n",
        "    names = None\n",
        "\n",
        "# 4) Load the model\n",
        "model, ckpt = None, None\n",
        "try:\n",
        "    from models.experimental import attempt_load\n",
        "    model = attempt_load(local_pt, map_location=\"cpu\", inplace=True, fuse=True)  # robust path\n",
        "    log(\"Loaded model via attempt_load ✅\")\n",
        "except Exception as e:\n",
        "    log(f\"attempt_load failed ({e}); trying safe torch.load\")\n",
        "    ckpt = torch.load(local_pt, map_location=\"cpu\", weights_only=False)  # trusted file\n",
        "    model = ckpt.get(\"ema\") or ckpt.get(\"model\")\n",
        "    assert model is not None, \"Checkpoint missing 'model' or 'ema'.\"\n",
        "\n",
        "# If names still unknown, try inside the model then fallback to MANUAL_NAMES\n",
        "if names is None:\n",
        "    if hasattr(model, \"names\") and model.names:\n",
        "        names = list(model.names.values()) if isinstance(model.names, dict) else list(model.names)\n",
        "        names = [str(n) for n in names]\n",
        "        log(f\"Using class names from checkpoint: {names}\")\n",
        "    else:\n",
        "        names = [str(n) for n in MANUAL_NAMES]\n",
        "        log(f\"Using MANUAL_NAMES fallback: {names}\")\n",
        "\n",
        "# Build names mapping with BOTH int and str keys (Addax expects string indexing too)\n",
        "name_map = {i: n for i, n in enumerate(names)}\n",
        "name_map.update({str(i): n for i, n in enumerate(names)})\n",
        "setattr(model, \"names\", name_map)\n",
        "setattr(model, \"nc\", len(names))\n",
        "\n",
        "# Optional: fuse layers (may already be fused) and set to eval/float\n",
        "try:\n",
        "    if hasattr(model, \"fuse\"):\n",
        "        model.fuse()\n",
        "except Exception:\n",
        "    pass\n",
        "try:\n",
        "    model.float().eval()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 5) Sanitize: recursively convert ANY pathlib.Path/PurePath to strings\n",
        "def to_serializable(obj, depth=0, max_depth=10):\n",
        "    if depth > max_depth:\n",
        "        return obj\n",
        "    try:\n",
        "        if isinstance(obj, (pathlib.Path, pathlib.PurePath)):\n",
        "            return str(obj)\n",
        "        if isinstance(obj, dict):\n",
        "            return {to_serializable(k, depth+1): to_serializable(v, depth+1) for k, v in obj.items()}\n",
        "        if isinstance(obj, (list, tuple, set)):\n",
        "            T = type(obj)\n",
        "            return T(to_serializable(v, depth+1) for v in obj)\n",
        "        if hasattr(obj, \"__dict__\") and not isinstance(obj, torch.nn.Module):\n",
        "            for k in list(vars(obj).keys()):\n",
        "                try:\n",
        "                    setattr(obj, k, to_serializable(getattr(obj, k), depth+1))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            return obj\n",
        "    except Exception:\n",
        "        return obj\n",
        "    return obj\n",
        "\n",
        "# Clean checkpoint dict\n",
        "clean = ckpt if isinstance(ckpt, dict) else {}\n",
        "clean[\"model\"] = model\n",
        "\n",
        "# Common attributes that might contain Path objects\n",
        "for attr in (\"yaml\", \"cfg\", \"args\"):\n",
        "    if hasattr(model, attr):\n",
        "        try:\n",
        "            setattr(model, attr, to_serializable(getattr(model, attr)))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# Sanitize entire checkpoint\n",
        "for k in list(clean.keys()):\n",
        "    clean[k] = to_serializable(clean[k])\n",
        "\n",
        "# Strip training-only baggage\n",
        "for k in (\"optimizer\", \"updates\", \"wandb_id\", \"training_results\", \"ema\"):\n",
        "    if k in clean:\n",
        "        clean[k] = None\n",
        "clean[\"epoch\"] = clean.get(\"epoch\", -1)\n",
        "clean[\"best_fitness\"] = clean.get(\"best_fitness\", None)\n",
        "clean[\"date\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Save as Windows-safe Addax file\n",
        "stem = Path(SRC).stem\n",
        "OUT = os.path.join(SAVE_DIR, f\"{stem}_addaxwin.pt\")\n",
        "torch.save(clean, OUT)\n",
        "\n",
        "# Quick verification\n",
        "def md5(p):\n",
        "    h = hashlib.md5()\n",
        "    with open(p, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "chk = torch.load(OUT, map_location=\"cpu\", weights_only=False)\n",
        "m2 = chk[\"model\"]\n",
        "ok_names = (m2.names[0] == names[0]) and (m2.names[\"0\"] == names[0]) and (getattr(m2, \"nc\", None) == len(names))\n",
        "\n",
        "print(\"\\n================= DONE =================\")\n",
        "print(\"Saved Addax file :\", OUT)\n",
        "print(\"Size (MB)        :\", round(os.path.getsize(OUT)/1e6, 2), \"| MD5:\", md5(OUT))\n",
        "print(\"Class check      :\", \"OK\" if ok_names else \"CHECK FAILED\")\n",
        "print(\"Classes          :\", names)\n",
        "print(\"Upload this file to Addax. (It includes both int & str keys in model.names and no PosixPath.)\")\n"
      ]
    }
  ]
}